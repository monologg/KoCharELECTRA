# KoCharELECTRA

**Character-level (ìŒì ˆ)** Korean ELECTRA Model

## Details

Wordpiece-levelì´ ì•„ë‹Œ **Character-level(ìŒì ˆ) tokenizer**ë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµí•œ í•œêµ­ì–´ ELECTRA Modelì…ë‹ˆë‹¤.

|       **Model**       | Max Len | Vocab Size |
| :-------------------: | ------: | ---------: |
| `KoCharElectra-Base`  |     512 |      11568 |
| `KoCharElectra-Small` |     512 |      11568 |

- Vocabì˜ ì‚¬ì´ì¦ˆëŠ” `11568`ê°œë¡œ `[unused]` í† í° 200ê°œë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.
- `í•œì`ì˜ ê²½ìš°ëŠ” ì „ì²˜ë¦¬ ì‹œì— ì œì™¸ë˜ì–´ Vocabì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

## Tokenizer

- Char-level tokenizerë¥¼ ìœ„í•˜ì—¬ `tokenization_kocharelectra.py` íŒŒì¼ì„ ìƒˆë¡œ ì œì‘
- Transformersì˜ tokenization ê´€ë ¨ í•¨ìˆ˜ ì§€ì› (`convert_tokens_to_ids`, `convert_tokens_to_string`, `encode_plus`...)

```python
>>> from tokenization_kocharelectra import KoCharElectraTokenizer
>>> tokenizer = KoCharElectraTokenizer.from_pretrained("monologg/kocharelectra-base-discriminator")
>>> tokenizer.tokenize("ë‚˜ëŠ” ê±¸ì–´ê°€ê³  ìˆëŠ” ì¤‘ì…ë‹ˆë‹¤.")
['ë‚˜', 'ëŠ”', ' ', 'ê±¸', 'ì–´', 'ê°€', 'ê³ ', ' ', 'ìˆ', 'ëŠ”', ' ', 'ì¤‘', 'ì…', 'ë‹ˆ', 'ë‹¤', '.']
```

## KoCharELECTRA on Transformers

- **Huggingface S3**ì— ëª¨ë¸ì´ ì´ë¯¸ ì—…ë¡œë“œë˜ì–´ ìˆì–´ì„œ, **ëª¨ë¸ì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•  í•„ìš” ì—†ì´** ê³§ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- `ElectraModel`ì€ `pooled_output`ì„ ë¦¬í„´í•˜ì§€ ì•ŠëŠ” ê²ƒì„ ì œì™¸í•˜ê³  `BertModel`ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤.

- ELECTRAëŠ” finetuningì‹œì— `discriminator`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

### ğŸš¨ ì£¼ì˜ì‚¬í•­ ğŸš¨

1. ë°˜ë“œì‹œ `Transformers v2.9.1` ì´ìƒì„ ì„¤ì¹˜í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤. (**v2.9.1ì—ì„œ ìƒˆë¡œ ë³€ê²½ëœ APIì— ë§ê²Œ tokenization íŒŒì¼ì„ ì œì‘í–ˆìŠµë‹ˆë‹¤**)

2. tokenizerì˜ ê²½ìš° wordpieceê°€ ì•„ë‹Œ char ë‹¨ìœ„ì´ê¸°ì— `ElectraTokenizer`ê°€ ì•„ë‹ˆë¼ `KoCharElectraTokenizer`ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. (ë ˆí¬ì—ì„œ ì œê³µí•˜ê³  ìˆëŠ” `tokenization_kocharelectra.py`ë¥¼ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.)

```python
from transformers import ElectraTokenizer  # DON'T use ElectraTokenizer
from tokenization_kocharelectra import KoCharElectraTokenizer  # USE KoCharElectraTokenizer
```

```python
from transformers import ElectraModel
from tokenization_kocharelectra import KoCharElectraTokenizer

# KoCharElectra-Base
model = ElectraModel.from_pretrained("monologg/kocharelectra-base-discriminator")
tokenizer = KoCharElectraTokenizer.from_pretrained("monologg/kocharelectra-base-discriminator")

# KoCharElectra-Small
model = ElectraModel.from_pretrained("monologg/kocharelectra-small-discriminator")
tokenizer = KoCharElectraTokenizer.from_pretrained("monologg/kocharelectra-small-discriminator")
```

### Testing Code

```bash
$ python3 test_kocharelectra.py

# Output
--------------------------------------------------------
tokens:  [CLS] ë‚˜ ëŠ”   ê±¸ ì–´ ê°€ ê³    ìˆ ëŠ”   ì¤‘ ì… ë‹ˆ ë‹¤ . [SEP] ë‚˜ ëŠ”   ë°¥ ì„   ë¨¹ ê³    ìˆ ëŠ”   ì¤‘ ì… ë‹ˆ ë‹¤ . [SEP]
input_ids: 2 40 8 5 374 38 14 13 5 36 8 5 75 142 57 7 10 3 40 8 5 733 11 5 445 13 5 36 8 5 75 142 57 7 10 3 0 0 0 0
token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0
attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0
--------------------------------------------------------
[Last layer hidden state]
Size: torch.Size([1, 40, 768])
Tensor: tensor([[[ 0.1453, -0.0629,  0.2065,  ...,  0.5304, -0.4602,  0.6803],
         [ 0.8824, -0.3448, -0.3342,  ...,  0.4652, -0.2378,  0.2560],
         [ 0.3114, -0.3019, -0.1159,  ...,  0.4712, -0.6678,  0.3425],
         ...,
         [-0.0830, -0.2008,  0.2107,  ..., -0.2890, -0.0297,  0.5241],
         [ 0.0587, -0.2498,  0.4193,  ..., -0.2537,  0.1526,  0.5394],
         [ 0.1337, -0.2736,  0.6251,  ..., -0.1580,  0.2323,  0.5248]]])
```

## Result on Subtask

Char-levelì¸ ê´€ê³„ë¡œ `max_seq_len`ì€ 128ì¸ ìµœëŒ€ ê¸¸ì´ë¡œ ëŒë ¸ì§€ë§Œ, KoELECTRAì™€ ë¹„êµí–ˆì„ ë•Œ ë‚˜ì˜ì§€ ì•Šì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

### Base Model

|                        | NSMC (acc) | Naver NER (F1) |
| ---------------------- | :--------: | :------------: |
| KoELECTRA-Base         |   90.21    |     86.87      |
| **KoCharELECTRA-Base** | **90.18**  |   **84.52**    |

### Small Model

|                         | NSMC (acc) | Naver NER (F1) |
| ----------------------- | :--------: | :------------: |
| KoELECTRA-Small         |   88.76    |     84.11      |
| **KoCharELECTRA-Small** | **89.20**  |   **82.83**    |

## Acknowledgement

KoCharELECTRAì€ **Tensorflow Research Cloud (TFRC)** í”„ë¡œê·¸ë¨ì˜ Cloud TPU ì§€ì›ìœ¼ë¡œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.

## Reference

- [ELECTRA](https://github.com/google-research/electra)
- [Huggingface Transformers](https://github.com/huggingface/transformers)
- [Tensorflow Research Cloud](https://www.tensorflow.org/tfrc?hl=ko)
